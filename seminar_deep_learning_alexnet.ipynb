{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "seminar_deep_learning_alexnet.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "QZ0afYAl0_GK"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhhjoSkBQxy3",
        "colab_type": "text"
      },
      "source": [
        "**Seminar Deep Learning - AlexNet - Friedrch, Leon**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZ0afYAl0_GK",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYB3Jm6n21t8",
        "colab_type": "text"
      },
      "source": [
        "This notebook allows you to test an implementation of the well known 'AlexNet', look at the learned kernels and see what effect these kernels have on the input images. \n",
        "\n",
        "As input data the images of the 'Sweaty' were used. The dataset consist of roughly 5000 training and 1200 test images in one of the following eight categories: \\\\\n",
        "0: Ball \\\\\n",
        "1: Post \\\\\n",
        "2: Obstacle \\\\\n",
        "3: L-line \\\\\n",
        "4: X-Line \\\\\n",
        "5: T-Line \\\\\n",
        "6: 11m-point \\\\\n",
        "7: Foot \\\\\n",
        "The dataset can be found in the moodle course 'Semiar-Deep-Learning-WS19'.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKN6XAqBPj0J",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxxnZVjK0xH-",
        "colab_type": "text"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "590iQnwWy7c2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "from PIL import Image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojOL9SNSPsLn",
        "colab_type": "text"
      },
      "source": [
        "Mount google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUKETPqDqT2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv19OKwuP7ku",
        "colab_type": "text"
      },
      "source": [
        "Copy and unzip data \\\\\n",
        "\n",
        "For this to work, the data needs to be stored in your google drive in a folder called 'Daten_zip'. Copying one 'zip' file turned out to be much faster than copying multiple images. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E05asxTMrkRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Change the path to the location of your data.\n",
        "!cp -r \"/content/gdrive/My Drive/Daten_zip/\" ./zip_img \n",
        "!unzip zip_img/magmaDataSet.zip -d zip_img/input_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5knOqLAQES9",
        "colab_type": "text"
      },
      "source": [
        "Set directories\n",
        "\n",
        "Here some constants are used to describe paths to the data and to folders where the model and images are saved."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74EIKd_Sy7dU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# modify this to point to your data directory\n",
        "DATA_DIR = 'zip_img'\n",
        "INPUT_DIR = DATA_DIR + '/input_data'\n",
        "TRAIN_IMG_DIR = INPUT_DIR + '/train'\n",
        "TEST_IMG_DIR = INPUT_DIR + '/test'\n",
        "OUTPUT_DIR = DATA_DIR + '/output'\n",
        "CHECKPOINT_DIR = OUTPUT_DIR + '/models'  # model checkpoints\n",
        "SAVE_IMAGE_DIR = OUTPUT_DIR + '/images'  # wrongly classified images\n",
        "\n",
        "# make checkpoint path directory\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "os.makedirs(SAVE_IMAGE_DIR, exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FdpJzSAQLDe",
        "colab_type": "text"
      },
      "source": [
        "Define hyperparameters "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnOV8gUmy7eV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define pytorch device \n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# define model parameters\n",
        "NUM_EPOCHS = 20 # original paper: 90\n",
        "BATCH_SIZE = 128\n",
        "MOMENTUM = 0.9\n",
        "LR_DECAY = 0.0005\n",
        "LR_INIT = 0.01\n",
        "IMAGE_DIM = 96 # pixels (original paper: 227)\n",
        "NUM_CLASSES = 8 # original paper: 1000 classes for imagenet 2012 dataset\n",
        "DEVICE_IDS = [0]  # GPUs to use"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kR2r2b5QRnf",
        "colab_type": "text"
      },
      "source": [
        "# Implementations of alexnet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I62DsUY03iDX",
        "colab_type": "text"
      },
      "source": [
        "There can be found two different versions of the 'AlexNet' below. The first one called AlexNet was build according to the paper 'ImageNet Classification with Deep Convolutional Neural Networks'. The second one has the same parameters as the pytorch version of the 'AlexNet'.  \n",
        "These two implementations have the same overall structure but have slightly different parameters in some layers.\n",
        "The pytorch version additionally offers a pretrained version of the network, which can be used, as well.\n",
        "\n",
        "For visualization purposes the networks are able to pass the input data only through a defined number of convolutional layers. To do so, an additional parameter is passed to the 'forward()' specifying the desired layer. The default value is '0' meaning all layers are used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4506GF8BvtI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AlexNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Neural network model consisting of layers propsed by AlexNet paper.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=8):\n",
        "        \"\"\"\n",
        "        Define and allocate layers for this neural net.\n",
        "        Args:\n",
        "            num_classes (int): number of classes to predict with this model\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(96, 256, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(256, 384, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(384, 384, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(384, 256, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        # classifier is just a name for linear layers\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5, inplace=True),\n",
        "            nn.Linear(in_features=(256 * 1 * 1), out_features=4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5, inplace=True),\n",
        "            nn.Linear(in_features=4096, out_features=4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=4096, out_features=num_classes),\n",
        "        )\n",
        "        self.init_bias()  # initialize bias\n",
        "\n",
        "    def init_bias(self):\n",
        "        for layer in self.features:\n",
        "            if isinstance(layer, nn.Conv2d):\n",
        "                nn.init.normal_(layer.weight, mean=0, std=0.01)\n",
        "                nn.init.constant_(layer.bias, 0)\n",
        "        # original paper = 1 for Conv2d layers (2nd, 4th, and 5th conv layer), but leads to worse results in my case.\n",
        "        #nn.init.constant_(self.features[4].bias, 1)\n",
        "        #nn.init.constant_(self.features[10].bias, 1)\n",
        "        #nn.init.constant_(self.features[12].bias, 1)\n",
        "\n",
        "    def forward(self, x, layer = 0):\n",
        "        \"\"\"\n",
        "        Pass the input through the net.\n",
        "        Args:\n",
        "            x (Tensor): input tensor\n",
        "            layer (int): number of convolutional layers, the input is passed through. Used for visualization.\n",
        "        Returns:\n",
        "            output (Tensor): output tensor\n",
        "        \"\"\"\n",
        "        x = self.features[0](x) # nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=0)\n",
        "        if layer == 1:\n",
        "            return x\n",
        "        x = self.features[1](x) # nn.ReLU()\n",
        "        x = self.features[2](x) # nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2)\n",
        "        x = self.features[3](x) # nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        x = self.features[4](x) # nn.Conv2d(96, 256, 5, padding=2)\n",
        "        if layer == 2:\n",
        "            return x\n",
        "        x = self.features[5](x) # nn.ReLU()\n",
        "        x = self.features[6](x) # nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2)\n",
        "        x = self.features[7](x) # nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        x = self.features[8](x) # nn.Conv2d(256, 384, 3, padding=1)\n",
        "        if layer == 3:\n",
        "            return x\n",
        "        x = self.features[9](x) # nn.ReLU()\n",
        "        x = self.features[10](x) # nn.Conv2d(384, 384, 3, padding=1)\n",
        "        if layer == 4:\n",
        "            return x\n",
        "        x = self.features[11](x) # nn.ReLU()\n",
        "        x = self.features[12](x) # nn.Conv2d(384, 256, 3, padding=1)\n",
        "        if layer == 5:\n",
        "            return x\n",
        "        x = self.features[13](x) # nn.ReLU()\n",
        "        x = self.features[14](x) # nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "\n",
        "        x = x.view(-1, 256 * 1 * 1)  # reduce the dimensions for linear layer input\n",
        "\n",
        "        return self.classifier(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kqwGSPpc-r3p",
        "colab": {}
      },
      "source": [
        "class AlexNetPytorch(nn.Module):\n",
        "    \"\"\"\n",
        "    Neural network model consisting of layers according to the pytorch alexnet. \n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=8):\n",
        "        \"\"\"\n",
        "        Define and allocate layers for this neural net.\n",
        "        Args:\n",
        "            num_classes (int): number of classes to predict with this model\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
        "            nn.Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
        "            nn.Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
        "        )\n",
        "        # classifier is just a name for linear layers\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5, inplace=False),\n",
        "            nn.Linear(in_features=256*2*2, out_features=4096, bias=True),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5, inplace=False),\n",
        "            nn.Linear(in_features=4096, out_features=4096, bias=True),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=4096, out_features=num_classes, bias=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, layer = 0):\n",
        "        \"\"\"\n",
        "        Pass the input through the net.\n",
        "        Args:\n",
        "            x (Tensor): input tensor\n",
        "            layer (int): number of convolutional layers, the input is passed through. Used for visualization.\n",
        "        Returns:\n",
        "            output (Tensor): output tensor\n",
        "        \"\"\"\n",
        "        x = self.features[0](x) # nn.Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
        "        if layer == 1:\n",
        "            return x\n",
        "        x = self.features[1](x) # nn.ReLU()\n",
        "        x = self.features[2](x) # nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "        x = self.features[3](x) # nn.Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
        "        if layer == 2:\n",
        "            return x\n",
        "        x = self.features[4](x) # nn.ReLU()\n",
        "        x = self.features[5](x) # nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "        x = self.features[6](x) # nn.Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        if layer == 3:\n",
        "            return x\n",
        "        x = self.features[7](x) # nn.ReLU()\n",
        "        x = self.features[8](x) # nn.Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        if layer == 4:\n",
        "            return x\n",
        "        x = self.features[9](x) # nn.ReLU()\n",
        "        x = self.features[10](x) # nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        if layer == 5:\n",
        "            return x\n",
        "        x = self.features[11](x) # nn.ReLU()\n",
        "        x = self.features[12](x) # nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "\n",
        "        x = x.view(-1, 256 * 2 * 2)  # reduce the dimensions for linear layer input\n",
        "\n",
        "        return self.classifier(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lL-IdOqbl_T",
        "colab_type": "text"
      },
      "source": [
        "# Select alexnet version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4i9KMtC6vOn",
        "colab_type": "text"
      },
      "source": [
        "In this section you can choose the version you want to use. Add a comment to the one you do not want to use. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHfwbwzWbqsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alexnet = AlexNet(num_classes=NUM_CLASSES).to(device)\n",
        "#alexnet = AlexNetPytorch(num_classes=NUM_CLASSES).to()device"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voKbHcy4Q3tb",
        "colab_type": "text"
      },
      "source": [
        "**Use pretrained Model**\n",
        "\n",
        "Remove the comment, if you want to use the pretrained pytorch version. \n",
        "\n",
        "A pretrained model is downloaded, the number of weights in the layers is adapted and afterwards the pretrained weights are copied to a new instance of 'AlexNetPytorch'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeL8rGt0eDRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model = models.alexnet(pretrained=True).to(device)\n",
        "#model.classifier[1] = nn.Linear(model.classifier[1].in_features, 256 * 2 * 2).to(device)\n",
        "#model.classifier[1] = nn.Linear(model.classifier[1].out_features, model.classifier[4].in_features).to(device)\n",
        "#model.classifier[6] = nn.Linear(model.classifier[6].in_features, NUM_CLASSES).to(device)\n",
        "#torch.save(model.state_dict(), 'pretrained_model')\n",
        "#alexnet = AlexNetPytorch(num_classes=NUM_CLASSES)\n",
        "#alexnet.load_state_dict(torch.load('pretrained_model'))\n",
        "#alexnet.to(device)\n",
        "#!rm pretrained_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyC4w6mWQXJ8",
        "colab_type": "text"
      },
      "source": [
        "# Train selected net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsYYyKchy7ek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    # print the seed value\n",
        "    seed = torch.initial_seed()\n",
        "    print('Used seed : {}'.format(seed))\n",
        "\n",
        "    print(alexnet)\n",
        "\n",
        "    # create dataset and data loader for train images\n",
        "    dataset = datasets.ImageFolder(TRAIN_IMG_DIR, transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]))\n",
        "    dataloader = data.DataLoader(\n",
        "        dataset,\n",
        "        shuffle=True,\n",
        "        pin_memory=True,\n",
        "        num_workers=8,\n",
        "        drop_last=True,\n",
        "        batch_size=BATCH_SIZE)\n",
        "\n",
        "    # create dataset and data loader for test images\n",
        "    dataset_test = datasets.ImageFolder(TEST_IMG_DIR, transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]))\n",
        "    dataloader_test = data.DataLoader(\n",
        "        dataset_test,\n",
        "        shuffle=False,\n",
        "        pin_memory=True,\n",
        "        num_workers=8,\n",
        "        drop_last=True,\n",
        "        batch_size=BATCH_SIZE)\n",
        "\n",
        "    print('Datasets and dataloaders created.')\n",
        "        \n",
        "\n",
        "    # create optimizer\n",
        "    optimizer = optim.Adam(params=alexnet.parameters(), lr=0.0001)\n",
        "    ### BELOW is the setting proposed by the original paper - which doesn't train...\n",
        "    #optimizer = optim.SGD(\n",
        "        #params=alexnet.parameters(),\n",
        "        #lr=LR_INIT,\n",
        "        #momentum=MOMENTUM,\n",
        "        #weight_decay=LR_DECAY)\n",
        "    print('Optimizer created')\n",
        "\n",
        "    # multiply LR by 1 / 10 after every 30 epochs\n",
        "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "    print('LR Scheduler created')\n",
        "\n",
        "    # start training\n",
        "    print('Starting training...')\n",
        "    total_steps = 1\n",
        "    loss_values = []\n",
        "    accuracy_values = []\n",
        "    loss_test = []\n",
        "    accuracy_test = []\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        correct_predictions = 0\n",
        "        steps = 0\n",
        "        for imgs, classes in dataloader:\n",
        "            imgs, classes = imgs.to(device), classes.to(device)\n",
        "\n",
        "            # calculate the loss\n",
        "            output = alexnet(imgs)\n",
        "            loss = F.cross_entropy(output, classes)\n",
        "            loss_values.append(loss.item())\n",
        "            _, preds = torch.max(output, 1)\n",
        "            temp_accuracy = 100 * (torch.sum(preds == classes).item() / classes.shape[0] )\n",
        "            accuracy_values.append(temp_accuracy)\n",
        "            correct_predictions += torch.sum(preds == classes).item()\n",
        "\n",
        "            # update the parameters\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_steps += 1\n",
        "            steps += imgs.shape[0]\n",
        "\n",
        "\n",
        "# Calculation of test loss and accuracy not possible, since cuda memory is not enough. Runtime error: cuda out of memory\n",
        "        #for imgs_test, classes_test in dataloader_test:\n",
        "            #imgs_test, classes_test = imgs_test.to(device), classes_test.to(device)\n",
        "\n",
        "            ## calculate the loss\n",
        "            #output_test = alexnet(imgs_test)\n",
        "            #loss_test_temp = F.cross_entropy(output_test, classes_test)\n",
        "            #loss_test.append(loss_test_temp.item())\n",
        "            #_, preds_test = torch.max(output_test, 1)\n",
        "            #temp_accuracy = 100 * (torch.sum(preds_test == classes_test).item() / classes_test.shape[0] )\n",
        "            #accuracy_test.append(temp_accuracy)\n",
        "\n",
        "        # print the epoch, accuracy and loss\n",
        "\n",
        "        loss = F.cross_entropy(output, classes)\n",
        "        acc = 100 * correct_predictions / steps\n",
        "        print('epoch: {} \\t loss: {:.2f} \\t accuracy: {:.2f}'.format(epoch, loss.item(), acc))\n",
        "\n",
        "        # save checkpoint every 20 epochs\n",
        "        if ( epoch % 20 == 0):\n",
        "            checkpoint_path = os.path.join(CHECKPOINT_DIR, 'alexnet_states_e{}.pkl'.format(epoch + 1))\n",
        "            state = {\n",
        "                'epoch': epoch,\n",
        "                'total_steps': total_steps,\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'model': alexnet.state_dict(),\n",
        "                'seed': seed,\n",
        "            }\n",
        "            torch.save(state, checkpoint_path)\n",
        "\n",
        "        lr_scheduler.step()\n",
        "    checkpoint_path = os.path.join(CHECKPOINT_DIR + '/trained_model.pkl')\n",
        "    state = {\n",
        "        'epoch': epoch,\n",
        "        'total_steps': total_steps,\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'model': alexnet.state_dict(),\n",
        "        'seed': seed,\n",
        "    }\n",
        "    torch.save(state, checkpoint_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hepr2oQ_NHv5",
        "colab_type": "text"
      },
      "source": [
        "Plot accuracy and loss for test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0i7I3Na0FYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig_acc = plt.figure()\n",
        "plt.plot(np.arange(len(accuracy_values)), accuracy_values)\n",
        "ax_acc = fig_acc.add_subplot()\n",
        "ax_acc.set_title('Accuracy')\n",
        "ax_acc.set_xlabel('Steps')\n",
        "\n",
        "fig_loss = plt.figure()\n",
        "plt.plot(np.arange(len(loss_values)), loss_values)\n",
        "ax_loss = fig_loss.add_subplot()\n",
        "ax_loss.set_title('Loss')\n",
        "ax_loss.set_xlabel('Steps')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jO7ltH0R5fP",
        "colab_type": "text"
      },
      "source": [
        "# Calculate test error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZsndZOv8Gu2",
        "colab_type": "text"
      },
      "source": [
        "Since the calculation and plotting of the test accuracy and loss are not possible due to a cuda memory issue, the test accuracy is calculated below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3T4ZVl0480F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = datasets.ImageFolder(TEST_IMG_DIR, transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "]))\n",
        "dataset_show = datasets.ImageFolder(TEST_IMG_DIR, transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "]))\n",
        "print('Dataset created')\n",
        "dataloader = data.DataLoader(\n",
        "    dataset,\n",
        "    shuffle=False,\n",
        "    pin_memory=True,\n",
        "    num_workers=8,\n",
        "    drop_last=True,\n",
        "    batch_size=1\n",
        ")\n",
        "dataloader_show = data.DataLoader(\n",
        "    dataset_show,\n",
        "    shuffle=False,\n",
        "    pin_memory=True,\n",
        "    num_workers=8,\n",
        "    drop_last=True,\n",
        "    batch_size=1\n",
        ")\n",
        "\n",
        "images = []\n",
        "labels = []\n",
        "images_show = []\n",
        "labels_show = []\n",
        "\n",
        "for i, l in dataloader:\n",
        "    images.append(i.to(device))\n",
        "    labels.append(l.to(device))\n",
        "\n",
        "for i, l in dataloader_show:\n",
        "    images_show.append(i.to(device))\n",
        "    labels_show.append(l.to(device))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVjtkWSwokYM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "errors = []\n",
        "wrong_pred = []\n",
        "right_pred = []\n",
        "wrong_imgs = []\n",
        "for im in range(len(images)):\n",
        "    #output = alexnet(images[im])\n",
        "    output = alexnet(images[im])\n",
        "    if output.argmax() != labels[im]:\n",
        "        wrong_pred.append(output.argmax())\n",
        "        right_pred.append(labels_show[im])\n",
        "        errors.append(im)\n",
        "        wrong_imgs.append(images_show[im])\n",
        "\n",
        "error = 100 * len(wrong_imgs) / len(images_show)\n",
        "print('Test error: {:.2f}'.format(error))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ltJtD_bkFBD",
        "colab_type": "text"
      },
      "source": [
        "# Plot error images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnJDnhyj8d-Z",
        "colab_type": "text"
      },
      "source": [
        "To learn something about the problems the network had when learning the classification, it can be interessting to have a look at the wrongly classified images. These are plotted below and saved in the specified output folder (SAVE_IMAGE_DIR).\n",
        "The images are labeled with the wrong prediction (w) and the ground truth (r)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DazG6neo5g3k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ncols = 20;\n",
        "nrows = (len(wrong_imgs) // ncols) + 1\n",
        "count = 0;\n",
        "label_names = {0:'ball', 1:'post', 2:'obstacle', 3:'L-line', 4:'X-line', 5:'T-line', 6:'11m-point', 7:'roboter_foot'}\n",
        "for idx in range(len(wrong_imgs)):\n",
        "    count += 1\n",
        "    fig1 = plt.figure()\n",
        "    ax1 = plt.subplot()\n",
        "    ax1.imshow(wrong_imgs[idx].cpu().squeeze().permute(1, 2, 0))\n",
        "    filename = 'w_{}_r_{}'.format(label_names[wrong_pred[idx].item()], label_names[right_pred[idx].item()])\n",
        "    fig1.savefig('{}/{}_{}.png'.format(SAVE_IMAGE_DIR, filename, count))\n",
        "    ax1.set_title('{}.\\nw: {} \\nr: {}'.format(count, label_names[wrong_pred[idx].item()], label_names[right_pred[idx].item()]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nY2mxCkszK2",
        "colab_type": "text"
      },
      "source": [
        "**Copy written images to google drive**\n",
        "\n",
        "Run this block if you want to copy the wrongly classified images to your google drive.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K97hTeWE-S93",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!zip output_imgs.zip zip_img/output/images/*.png\n",
        "!cp output_imgs.zip /content/gdrive/My\\ Drive/Daten/output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOEHFZMQS2bS",
        "colab_type": "text"
      },
      "source": [
        "# Visualize learned kernels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyziEni-9PK0",
        "colab_type": "text"
      },
      "source": [
        "In this section the learned kernels of the network can be visualized using the method plot_weights().\n",
        "\n",
        "This method allows you to plot the kernels of a specified network and specified convolutional layer. If the chosen layer has three input channels, these three channels can be plotted as rgb image (single_channel = False).\n",
        "If the number of kernels in the specified layer exceeds 500, only a subset of filters is plotted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qv-nJbDFuNuN",
        "colab": {}
      },
      "source": [
        "def plot_filters_multi_channel(t):\n",
        "    \n",
        "    #get the number of kernals\n",
        "    num_kernels = t.shape[0]    \n",
        "    \n",
        "    #define number of columns for subplots\n",
        "    num_cols = 12\n",
        "    #rows = num of kernels\n",
        "    num_rows = num_kernels\n",
        "    \n",
        "    #set the figure size\n",
        "    fig = plt.figure(figsize=(num_cols,num_rows))\n",
        "    \n",
        "    #looping through all the kernels\n",
        "    for i in range(t.shape[0]):\n",
        "        ax1 = fig.add_subplot(num_rows,num_cols,i+1)\n",
        "        \n",
        "        #for each kernel, we convert the tensor to numpy \n",
        "        npimg = np.array(t[i].numpy(), np.float32)\n",
        "        #standardize the numpy image\n",
        "        npimg = (npimg - np.mean(npimg)) / np.std(npimg)\n",
        "        npimg = np.minimum(1, np.maximum(0, (npimg + 0.5)))\n",
        "        npimg = npimg.transpose((1, 2, 0))\n",
        "        ax1.imshow(npimg)\n",
        "        ax1.axis('off')\n",
        "        ax1.set_title(str(i))\n",
        "        ax1.set_xticklabels([])\n",
        "        ax1.set_yticklabels([])\n",
        "        \n",
        "    plt.savefig('myimage.png', dpi=100)    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jOMOAkC2YKPI",
        "colab": {}
      },
      "source": [
        "def plot_filters_single_channel(t):\n",
        "    \n",
        "    #kernels depth * number of kernels\n",
        "    nplots = t.shape[0]*t.shape[1]\n",
        "    ncols = 12\n",
        "    \n",
        "    nrows = 1 + nplots//ncols\n",
        "    #convert tensor to numpy image\n",
        "    npimg = np.array(t.numpy(), np.float32)\n",
        "    \n",
        "    count = 0\n",
        "    fig = plt.figure(figsize=(ncols, nrows))\n",
        "\n",
        "    #looping through all the kernels in each channel\n",
        "    for i in range(t.shape[0]):\n",
        "        for j in range(t.shape[1]):\n",
        "            count += 1\n",
        "            ax1 = fig.add_subplot(nrows, ncols, count)\n",
        "            npimg = np.array(t[i, j].numpy(), np.float32)\n",
        "            npimg = (npimg - np.mean(npimg)) / np.std(npimg)\n",
        "            npimg = np.minimum(1, np.maximum(0, (npimg + 0.5)))\n",
        "            ax1.imshow(npimg)\n",
        "            ax1.set_title(str(i) + ',' + str(j))\n",
        "            ax1.axis('off')\n",
        "            ax1.set_xticklabels([])\n",
        "            ax1.set_yticklabels([])\n",
        "   \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cWmfCalUvzbS",
        "colab": {}
      },
      "source": [
        "def plot_weights(model, layer_num, single_channel = True):\n",
        "  \n",
        "  #extracting the model features at the particular layer number\n",
        "  layer = model.features[layer_num]\n",
        "  \n",
        "  #checking whether the layer is convolution layer or not \n",
        "  if isinstance(layer, nn.Conv2d):\n",
        "    #getting the weight tensor data\n",
        "    weight_tensor = layer.weight.data.cpu()\n",
        "\n",
        "    if single_channel:\n",
        "        if ( (weight_tensor.shape[0] * weight_tensor.shape[1]) > 500):\n",
        "            print(weight_tensor.shape)\n",
        "            weight_tensor = weight_tensor[0:100, 0:2, :, :]\n",
        "        plot_filters_single_channel(weight_tensor)\n",
        "    \n",
        "    else:\n",
        "      if weight_tensor.shape[1] == 3:\n",
        "        plot_filters_multi_channel(weight_tensor)\n",
        "      else:\n",
        "        print(\"Can only plot weights with three channels with single channel = False\")\n",
        "        \n",
        "  else:\n",
        "    print(\"Can only visualize layers which are convolutional\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oY9qbCOEP3Mn",
        "colab": {}
      },
      "source": [
        "# visualize weights for alexnet. The second parameter is the absolut number of the layer.\n",
        "# For an instance of the class AlexNet() the convolutional layers have the numbers 0, 4, 8, 10 and 12.\n",
        "\n",
        "plot_weights(alexnet, 0, single_channel = True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4uMzNMXsTuH",
        "colab_type": "text"
      },
      "source": [
        "# Visualize filtered images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEK9K2nfEhYk",
        "colab_type": "text"
      },
      "source": [
        "To see what effect a specific filter has on an input image, you can plot the result of a selected layer for a selected input image below.\n",
        "\n",
        "Select the index of the image to select one of the test images (1230 images).\n",
        "Select the layer according "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvVL9r_QscL4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "selected_image_idx = 100\n",
        "selected_layer = 0\n",
        "\n",
        "if isinstance(alexnet.features[selected_layer], nn.Conv2d):\n",
        "    output_for_layer = alexnet(images[selected_image_idx], selected_layer + 1)\n",
        "    print(output_for_layer.shape)\n",
        "    \n",
        "    ax = plt.subplot()\n",
        "    ax.imshow(images_show[selected_image_idx].cpu().squeeze().permute(1, 2, 0))\n",
        "    ax.set_title('Original image')\n",
        "    for k in range(output_for_layer.shape[1]):\n",
        "        fig = plt.figure()\n",
        "        ax1 = plt.subplot()\n",
        "        image = output_for_layer[0][k]\n",
        "        image = image.data.cpu()\n",
        "        ax1.imshow(image)\n",
        "        ax1.set_title('kernel: {}'.format(k))\n",
        "\n",
        "else :\n",
        "    print('Selected layer is not a convolutional layer')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}